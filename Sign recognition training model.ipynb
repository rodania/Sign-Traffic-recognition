{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import class_weight\n",
    "from skimage import io\n",
    "from skimage import transform\n",
    "from skimage import exposure\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "from importlib import reload\n",
    "from model import TrafficClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting images and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(basePath, csvPath):\n",
    "    # create empty lists to store image paths & labels\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # open csv file and git rid of first row\n",
    "    data = open(csvPath).read().split('\\n')[1:]\n",
    "    for (i, d) in enumerate(data):\n",
    "        if i > 0 and i % 1000 == 0:\n",
    "            print(\"[INFO] processed {} total images\".format(i))\n",
    "            \n",
    "        if len(d.split(',')) > 1:\n",
    "\n",
    "            # get the label of each image\n",
    "            labels.append(d.split(',')[-2])\n",
    "            # get the path of each image\n",
    "            path = basePath + d.split(',')[-1]\n",
    "            \n",
    "            # read the image\n",
    "            img = io.imread(path)\n",
    "            \n",
    "            # improve image contrast by applying Adaptive Histogram Equalization\n",
    "            img = exposure.equalize_adapthist(img, clip_limit=0.1)\n",
    "            \n",
    "            # resize the image to be 32x32 pixels, ignoring aspect ratio\n",
    "            img = transform.resize(img, (32,32))\n",
    "            \n",
    "            # add processsed image to the list\n",
    "            images.append(img)\n",
    "            \n",
    "    # convert the data and labels to NumPy arrays\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "        \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the training and testing CSV files\n",
    "basePath = 'images/'\n",
    "trainPath = \"images/Train.csv\"\n",
    "testPath = \"images/Test.csv\"\n",
    "\n",
    "# load the training and testing data\n",
    "print(\"[INFO] loading training and testing data...\")\n",
    "(trainX, trainY) = get_images(basePath, trainPath)\n",
    "(testX, testY) = get_images(basePath, testPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train data into trani dataset & validation dataset\n",
    "trainX, valX, trainY, valY = train_test_split(trainX, trainY, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset\n",
    "np.save('datasets/trainX', trainX)\n",
    "np.save('datasets/trainy', trainY)\n",
    "np.save('datasets/valX', valX)\n",
    "np.save('datasets/valY', valY)\n",
    "np.save('datasets/testX', testX)\n",
    "np.save('datasets/testY', testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.load('datasets/trainX.npy')\n",
    "trainY = np.load('datasets/trainY.npy')\n",
    "valX = np.load('datasets/valX.npy')\n",
    "valY = np.load('datasets/valY.npy')\n",
    "testX = np.load('datasets/testX.npy')\n",
    "testY = np.load('datasets/testY.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sign name file\n",
    "signNames = pd.read_csv('images/signnames.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data to the range of [0, 1]\n",
    "trainX = trainX.astype(\"float32\") / 255.0\n",
    "valX = valX.astype(\"float32\") / 255.0\n",
    "testX = testX.astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get uniqe label of data\n",
    "num_labels = len(np.unique(trainY))\n",
    "\n",
    "# create weighted classes to overcome the imbalance in classes\n",
    "weight = class_weight.compute_class_weight('balanced', np.unique(trainY), trainY)\n",
    "weight = {i : weight[i] for i in range(num_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert labels to categorical data in all datasets \n",
    "trainY = to_categorical(trainY, num_labels)\n",
    "valY = to_categorical(valY, num_labels)\n",
    "testY = to_categorical(testY, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "learning_rate = 0.0001\n",
    "batch_size = 32\n",
    "    \n",
    "# initialize the optimizer and compile the model\n",
    "print(\"[INFO] compiling model...\")\n",
    "model = TrafficClassifier.createCNN(width=32, height=32, depth=3, classes=43)\n",
    "optimizer = Adam(lr=learning_rate, decay=learning_rate / (epochs))\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "fit = model.fit(\n",
    "    aug.flow(trainX, trainY, batch_size=batch_size), \n",
    "    epochs=epochs,\n",
    "    validation_data=(valX, valY),\n",
    "    class_weight=weight,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(testX, testY, verbose = 0) \n",
    "print('Test Loss: ', score[0]) \n",
    "print('Test Accuracy: ', score[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fit.history['loss']) \n",
    "plt.plot(fit.history['val_loss']) \n",
    "plt.legend(['training', 'validation']) \n",
    "plt.title('Loss') \n",
    "plt.xlabel('epoch') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fit.history['accuracy']) \n",
    "plt.plot(fit.history['val_accuracy']) \n",
    "plt.legend(['training', 'validation']) \n",
    "plt.title('Accuracy') \n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"sign_recognition_final_model/model.pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
